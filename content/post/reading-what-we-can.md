---
date: 2024-04-10
draft: false
params:
  author: Dan Johansson
  ShowToc: true
  ShowReadingTime: true
title: Reading What We Can - The Gauntlet
---

While attending an Apart Research: [https://www.apartresearch.com/](https://www.apartresearch.com/) sprint/hackathon, I was introduced by Esben Kran: [https://twitter.com/EsbenKC](https://twitter.com/EsbenKC) to the ["Reading What We Can"](https://readingwhatwecan.com/) challenge. This challenge involves reading a combination of 20 books and articles over 20 days within your chosen learning path. In my case, it's the "ML Engineering & AI Safety" path as defined on the challenge website. The purpose of this challenge is to consume and learn from important material, but also to frame learning as a digestible challenge.

I'll be updating this post with short thoughts and summaries of each reading material as I progress. Hopefully, this inspires someone to take on the challenge or at least explore some of these readings!

## Reading List

* Day 1: [*Preventing an AI-related catastrophe*](https://80000hours.org/problem-profiles/artificial-intelligence/)
* Day 1: [The AI Revolution](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html)
* Day 2: [AGI Safety From First Principles](https://drive.google.com/file/d/1uK7NhdSKprQKZnRjU58X7NLA1auXlWHt/view)















## Day 1: [*Preventing an AI-related catastrophe*](https://80000hours.org/problem-profiles/artificial-intelligence/)

**Thoughts and Summary**

In his article on the 80,000 Hours website, [Benjamin Hilton](https://80000hours.org/author/benjamin-hilton/) introduces fundamental AI safety concepts and explores arguments surrounding the potential for an AI-driven catastrophe. This piece is an invaluable resource for those interested in the field – from technical researchers and policy analysts to the generally curious. Hilton's measured and informative approach makes this a must-read for anyone concerned about the future of humanity, where AI may pose an existential risk.

**Key Takeaways:**

* **Experts Weigh In:** A significant number of experts believe there's a genuine risk of AI causing a catastrophe. Though quantifying this probability is challenging and studies yield varied results, it's a concern meriting serious consideration.
* **The Power Dynamic:** Advanced AI systems could seek increasing power, raising numerous safety issues. Carsmith (2022)'s report on power-seeking AI dives deeper into these dangers ([https://arxiv.org/abs/2206.13353](https://arxiv.org/abs/2206.13353)).
* **Competitive Risks:** Market forces incentivize rapid deployment of powerful AI systems. Prioritizing speed over safety can create hazards.
* **Dual Alignment Challenge:**  Aligning AI systems to our goals is complex. We must also preempt misuse of these systems by individuals or groups with malicious intent.
* **Intelligence ≠ Morality:**  Human-level understanding of ethics does not guarantee an AI agent will act ethically. 

**Hilton's Call to Action:**

1. **Technical AI Safety Research:**  Contribute to ongoing research in ways that fit your skills and resources.
2. **AI Strategy/Policy:**  Help shape responsible AI development through careers that bridge research, policy, and the industry. 







## Day 1: [The AI Revolution](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html)

**Thoughts and summary:**

Tim Urban, founder of the blog [Wait but Why](https://waitbutwhy.com/), wrote this article describing his discovery of the AI safety issue in early 2015.  The piece focuses on philosophical arguments for AI safety and the evolution of human technology rather than  technical details. Urban's casual,  entertaining writing style sets his work apart from similar material I've read. While I'm not deeply familiar with philosophy, I found the article engaging and recommend it to anyone interested in AI safety, human progress, or philosophy – despite some now-outdated elements.  That outdatedness, I'd argue, actually underscores Urban's point.

For instance, Urban highlights the challenges that faced early Artificial Narrow Intelligence (ANI), such as understanding language or classifying images, in contrast to its superhuman abilities in chess and calculation. Remarkably, today's ANI easily accomplishes those image classification tasks, often surpassing humans.  This rapid shift exemplifies the point that technological advancement was hard to grasp even in 2015.


**Key Takeaways:**

* **Law of acceleration:** 
More advanced societies progress faster than less advanced ones due to factors like increased resources, knowledge sharing, and accumulated gains. We tend to underestimate future technological progress because we often anchor our predictions to the current rate of change, rather than accounting for exponential growth.

* **Understanding the scope of AGI/ASI and the scaling of computers:** 
"Nothing will make you appreciate human intelligence like learning about how unbelievably challenging it is to try to create a computer as smart as we are." However, once a computer achieves human-level intelligence (AGI), it gains significant advantages:
  1. Speed - Modern microprocessors are already 10 million times faster then the brain (In neuron activation speed compared to transistor). Allowing the computer to perform the same cognitive task as a human but simply much faster.
  2. Computers surpass humans in scaling potential (RAM, storage, parallel computation, global knowledge access). Physical limitations restrict humans, while computers can endlessly share knowledge, parallelize tasks, and avoid the need for rest. This dramatically accelerates their progress.

* **The *POTENTIAL* incomprehensible intellect of ASI:**
 The Law of Acceleration suggests that ASI may be closer than we think. When coupled with a computer's inherent capacity for scaling intelligence, an AGI's self-learning could quickly lead to ASI.  This could result in an intellect so far beyond our comprehension that it'd be akin to humans observing ants. ASI has the potential to be humanity's final invention, either leading to our extinction or providing solutions like immortality. I strongly recommend reading Tim's thoughts on [the life balance beam](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-2.html#:~:text=First%2C%20looking%20at%20history%2C%20we%20can%20see%20that%20life%20works%20like%20this%3A%20species%20pop%20up%2C%20exist%20for%20a%20while%2C%20and%20after%20some%20time%2C%20inevitably%2C%20they%20fall%20off%20the%20existence%20balance%20beam%20and%20land%20on%20extinction%E2%80%94) for his argument that ASI presents an existential choice between extinction and immortality.

* **The curse of flesh:**
Tim notes that evolution only optimized humans for survival long enough to reproduce and raise offspring – not  for lifespans of 60, 70, or 80+ years. W.B. Yeats' quote, “a soul fastened to a dying animal,” encapsulates this mismatch between our intellect and our physical limitations. The thought of being on the cusp of an intelligence explosion driven by ASI, yet having it arrive potentially a decade too late for my own lifetime, admittedly fills me with a sense of FOMO.  Perhaps the best we can do is to contribute meaningfully to humanity's advancement, even if we don't fully experience the fruits of that labor.



## Day 2: [AGI Safety From First Principles](https://drive.google.com/file/d/1uK7NhdSKprQKZnRjU58X7NLA1auXlWHt/view)


**Thoughts and summary:**

This reading by [Richard Ngo](https://www.richardcngo.com/) felt much more familiar to read since it was much more scientific in its writing than the previous two readings. Ngo introduced the "second species" argument, in which humans become the second most powerful entity on the planet. Ngo then proceeds to present many interesting arguments both for and against the "second species" event happening. The reading focuses more on general and philosophical arguments rather than technical ones, which makes it much more digestible for any reader. Ngo also introduces many additional concepts and readings which makes this report a highly valuable work in my opinion.  

**Key Takeaways / Points:**

* **Second species argument:**  Humans could lose their position as the most powerful entity on Earth and their ability to control future development.

* **Legg and Hutter definition of intelligence:** "Intelligence as the ability to achieve goals in a wide range of environments." AI can achieve high intelligence in two ways:
    1. **Task-Based Approach:** An agent is optimized for many specific tasks.
    2. **Generalization-Based Approach:** An agent can solve new tasks by generalizing from previous experience.

* **Understanding our evolution:**  "One potential obstacle to the generalization-based approach succeeding is the possibility that specific features of the ancestral environment, or of human brains, were necessary for general intelligence to arise". It's crucial to analyze what enabled our transition into social, intelligent beings capable of driving our own evolution. This may hold keys to replicating general intelligence in AI.

* **Recursive AI improvement:** Ngo suggests a scenario where many AGIs replicate themselves, find better training regimens, and spread this knowledge to create smarter AGIs. This network would drive improvement, not just a single AGI self-improving. He notes that "the retraining step may be bottlenecked by compute even if an AGI is able to design algorithmic improvements very fast," meaning an AGI's progress might still be tied to available computing power, unlike human intellect.

* **AGI alignment and retraining:**  "For an AGI to trust that its goals will remain the same under retraining will likely require it to solve many of the same problems that the field of AI safety is currently tackling". Ensuring a robust, aligned goal is difficult and a huge topic in AI safety. This would also apply to an AGI, indicating we might have solved alignment before an AGI can solve it for itself to enable recursive improvement.

* **Agency vs. previous definitions:** Ngo compares previous attempts to describe agency, such as mesa-optimizers (Hubinger et al.), to his own refined definition that outlines the cognitive abilities of a goal-oriented agent:
    1. **Self-Awareness:** Understands its place in the world and the impact of its actions.
    2. **Planning:** Considers a wide range of potential actions over the long term.
    3. **Consequentialism:** Chooses the plan likely to yield the most desirable outcome.
    4. **Scale:** Decisions account for large-scale, long-term effects. 
    5. **Coherence:** Internal unity drives the implementation of a single optimal plan.
    6. **Flexibility:** Able to adapt plans as the situation demands.

    Ngo sees these traits as a spectrum for evaluating AI agency, not binary indicators. 

* **Agency vs. intelligence:** Ngo argues that agency isn't an emergent property of highly intelligent systems but a set of capabilities intentionally developed during training.  

* **Moravec's paradox:** The observation that seemingly complex human cognitive skills are easily replicated by AI, while AI struggles with seemingly simpler tasks.

* **Environment and agency:** We might expect AI trained in a complex environment to develop agentic behavior, as long-term planning and adaptability would be useful. Similarly, a competitive environment could drive even greater agentic behavior if overcoming rivals becomes necessary. 

**Agency and Self-Improvement:** We should expect low-agentic but self-improving systems to become more agentic as they get better at achieving their goals. However, Ngo cautions that if an agent is trained in a highly competitive environment, it might internalize this competitiveness when deployed in the real world, hindering cooperation with other AGIs.

* **Defining Alignment:** Discussions about alignment often focus on methods for aligning AI with human values. But what does "aligned with human values" even mean? Ngo builds on previous work to define two types of alignment:
    * **Minimalist (Narrow) Alignment:**  Focuses on avoiding catastrophes by aligning AI with the values of its creators, who are hopefully well-intentioned.
    * **Maximalist (Ambitious) Alignment:**  Attempts to align AI with a universal interpretation of human values in general (if such a thing exists). 

* **Intentions vs. Outcomes:** Ngo emphasizes the distinction between an AI's intentions and the results of its actions. An AI might appear aligned with human values yet cause catastrophic harm due to misinterpreting our instructions. Therefore, an agent's intentions are crucial. Ngo's primary concern is that agents may become intelligent enough to fully understand our desires but simply choose not to follow them due to misaligned motivations learned during training.

* **The Orthogonality Thesis:** This thesis states that any level of intelligence can be combined with any goal, implying that intelligence doesn't necessitate alignment with human values. High-functioning psychopaths exemplify this: they understand that morality motivates others and can exploit this knowledge for manipulation, but they themselves lack those moral motivations. 

* **Inner vs. Outer Alignment:**  The best concise definition I've found: "Outer alignment is the problem of correctly evaluating AI behavior; inner alignment is the problem of making the AI's goals match those evaluations."

* **Designing for Alignment:** We should develop systems (reward functions, optimizers, training environments, etc.) that create selection pressures favoring agents who think in desirable ways, leading to beneficial motivations across a wide range of situations.  One example is how evolution shaped humans to develop altruistic behaviors.

* **AI's Scaling Advantage:** The compounding effect of technological progress has accelerated human advancement, even without significant biological evolution of our brains. AI, able to leverage both cumulative intelligence gains and direct investment in its own hardware (larger "brains," faster processing, etc.), will possess a significant scaling advantage that might lead to an even faster rate of improvement.

* **Arguments Against a Discontinuous Intelligence Takeoff:**
    1. AGI development is likely to be highly competitive, favoring continuous small improvements over any sudden, massive breakthrough.
    2. While the increase in compute availability is exponential, it's still a continuous process.
    3. Historically, technological progress has been largely continuous rather than marked by sharp discontinuities.

* **Methods for Transparent AI:**
    1. **Interpretability Tools:**  Allow us to analyze a model's inner workings and thought processes.
    2. **Training for Transparency:**  Incentivize agents to explain their reasoning, for example, through positive reward signals.
    3. **Inherently Interpretable Architectures:**  Use models like AlphaGo's search algorithm, which offers some insight into its decision-making based on the paths explored.