---
date: 2024-04-10
draft: false
params:
  author: Dan Johansson
  ShowToc: true
  ShowReadingTime: true
title: Reading What We Can - The Gauntlet
---

While attending an Apart Research: [https://www.apartresearch.com/](https://www.apartresearch.com/) sprint/hackathon, I was introduced by Esben Kran: [https://twitter.com/EsbenKC](https://twitter.com/EsbenKC) to the ["Reading What We Can"](https://readingwhatwecan.com/) challenge. This challenge involves reading a combination of 20 books and articles over 20 days within your chosen learning path. In my case, it's the "ML Engineering & AI Safety" path as defined on the challenge website. The purpose of this challenge is to consume and learn from important material, but also to frame learning as a digestible challenge.

I'll be updating this post with short thoughts and summaries of each reading material as I progress. Hopefully, this inspires someone to take on the challenge or at least explore some of these readings!

## Reading List

* Day 1: [*Preventing an AI-related catastrophe*](https://80000hours.org/problem-profiles/artificial-intelligence/)
* Day 2: 

## Day 1: [*Preventing an AI-related catastrophe*](https://80000hours.org/problem-profiles/artificial-intelligence/)

**Thoughts and Summary**

In his article on the 80,000 Hours website, [Benjamin Hilton](https://80000hours.org/author/benjamin-hilton/) introduces fundamental AI safety concepts and explores arguments surrounding the potential for an AI-driven catastrophe. This piece is an invaluable resource for those interested in the field – from technical researchers and policy analysts to the generally curious. Hilton's measured and informative approach makes this a must-read for anyone concerned about the future of humanity, where AI may pose an existential risk.

**Key Takeaways:**

* **Experts Weigh In:** A significant number of experts believe there's a genuine risk of AI causing a catastrophe. Though quantifying this probability is challenging and studies yield varied results, it's a concern meriting serious consideration.
* **The Power Dynamic:** Advanced AI systems could seek increasing power, raising numerous safety issues. Carsmith (2022)'s report on power-seeking AI dives deeper into these dangers ([https://arxiv.org/abs/2206.13353](https://arxiv.org/abs/2206.13353)).
* **Competitive Risks:** Market forces incentivize rapid deployment of powerful AI systems. Prioritizing speed over safety can create hazards.
* **Dual Alignment Challenge:**  Aligning AI systems to our goals is complex. We must also preempt misuse of these systems by individuals or groups with malicious intent.
* **Intelligence ≠ Morality:**  Human-level understanding of ethics does not guarantee an AI agent will act ethically. 

**Hilton's Call to Action:**

1. **Technical AI Safety Research:**  Contribute to ongoing research in ways that fit your skills and resources.
2. **AI Strategy/Policy:**  Help shape responsible AI development through careers that bridge research, policy, and the industry. 


