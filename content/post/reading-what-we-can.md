---
date: 2024-04-10
draft: false
params:
  author: Dan Johansson
  ShowToc: true
  ShowReadingTime: true
title: Reading What We Can - The Gauntlet
---

While attending an Apart Research: [https://www.apartresearch.com/](https://www.apartresearch.com/) sprint/hackathon, I was introduced by Esben Kran: [https://twitter.com/EsbenKC](https://twitter.com/EsbenKC) to the ["Reading What We Can"](https://readingwhatwecan.com/) challenge. This challenge involves reading a combination of 20 books and articles over 20 days within your chosen learning path. In my case, it's the "ML Engineering & AI Safety" path as defined on the challenge website. The purpose of this challenge is to consume and learn from important material, but also to frame learning as a digestible challenge.

I'll be updating this post with short thoughts and summaries of each reading material as I progress. Hopefully, this inspires someone to take on the challenge or at least explore some of these readings!

## Reading List

* Day 1: [*Preventing an AI-related catastrophe*](https://80000hours.org/problem-profiles/artificial-intelligence/)
* Day 1: [The AI Revolution](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html)















## Day 1: [*Preventing an AI-related catastrophe*](https://80000hours.org/problem-profiles/artificial-intelligence/)

**Thoughts and Summary**

In his article on the 80,000 Hours website, [Benjamin Hilton](https://80000hours.org/author/benjamin-hilton/) introduces fundamental AI safety concepts and explores arguments surrounding the potential for an AI-driven catastrophe. This piece is an invaluable resource for those interested in the field – from technical researchers and policy analysts to the generally curious. Hilton's measured and informative approach makes this a must-read for anyone concerned about the future of humanity, where AI may pose an existential risk.

**Key Takeaways:**

* **Experts Weigh In:** A significant number of experts believe there's a genuine risk of AI causing a catastrophe. Though quantifying this probability is challenging and studies yield varied results, it's a concern meriting serious consideration.
* **The Power Dynamic:** Advanced AI systems could seek increasing power, raising numerous safety issues. Carsmith (2022)'s report on power-seeking AI dives deeper into these dangers ([https://arxiv.org/abs/2206.13353](https://arxiv.org/abs/2206.13353)).
* **Competitive Risks:** Market forces incentivize rapid deployment of powerful AI systems. Prioritizing speed over safety can create hazards.
* **Dual Alignment Challenge:**  Aligning AI systems to our goals is complex. We must also preempt misuse of these systems by individuals or groups with malicious intent.
* **Intelligence ≠ Morality:**  Human-level understanding of ethics does not guarantee an AI agent will act ethically. 

**Hilton's Call to Action:**

1. **Technical AI Safety Research:**  Contribute to ongoing research in ways that fit your skills and resources.
2. **AI Strategy/Policy:**  Help shape responsible AI development through careers that bridge research, policy, and the industry. 







## Day 1: [The AI Revolution](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html)


- In order for someone to be transported into the future and die from the level of shock they’d experience, they have to go enough years ahead that a “die level of progress,” or a Die Progress Unit (DPU) has been achieved.

- Human history’s Law of Accelerating Returns. This happens because more advanced societies have the ability to progress at a faster rate than less advanced societies—because they’re more advanced. [The Singularity Is Near: When Humans Transcend Biology](https://www.amazon.com/gp/product/0143037889/ref=as_li_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0143037889&linkCode=as2&tag=wabuwh00-20&linkId=54Q62R5PYJBEENTP)

- When we do prediction on the future advancement of technology we do it based on the current rate of advancement instead of taking exponential growth into mind. 
- Progress usually comes in the form of S-curves where a paradigm shift occurs and then matures before the next explosive S-curve
  1. Slow growth (the early phase of exponential growth)
  2. Rapid growth (the late, explosive phase of exponential growth)
  3. A leveling off as the particular paradigm matures

- The fact is, if we’re being truly logical and expecting historical patterns to continue, we should conclude that much, much, much more should change in the coming decades than we intuitively expect.

- 2010 Flash crash created by advanced ANI should indicate the potential for catastrophic mistakes by AGI or ASI.

- Aaron Saenz sees it, our world’s ANI systems “are like the amino acids in the early Earth’s primordial ooze”—the inanimate stuff of life that, one unexpected day, woke up.

- Nothing will make you appreciate human intelligence like learning about how unbelievably challenging it is to try to create a computer as smart as we are.

- scientist Donald Knuth puts it, “AI has by now succeeded in doing essentially everything that requires ‘thinking’ but has failed to do most of what people and animals do ‘without thinking.' But given resent development in object detection and multimodal models have changed this. 

When the CPC per $1000 reaches human level $10^{16}$ AGI will be at least computationally since one can match the human level of computational power for a low cost. 

- An AI agent with human level intelligence have significant advantages.
  1. Speed - Modern microprocessors are already 10 million times faster then the brain
  2. Computer scales faster then humans (RAM, Storage, parallel compute, Global knowledge store...)
  3. Lack of rest. A computer can run 24/7 the brain cant

- Just as a chimp cant even grasp many human concepts an AGI just a few steps above us on the "intelligence" ladder would look at us as we look at chimps. An ASI then would then be so intelligent that "there is no way to know what ASI will do or what the consequences will be for us."

- If evolution could be seen as a complex dynamical system for all species there is un undeniable state that exist, extinction is a attracting state that 99.9% of all species have transitioned to. Could ASI be the catalyst for a second attracting state, immortality? 
  - "When are we going to hit the tripwire and which side of the beam will we land on when that happens?"

- ASI would be so advanced to us that it would analogous to us looking down at ants.

- Since evolution see no reason for us to live longer then to reproduce and raise our children. Humans were never evolutionary made to live 60,70,80+ years. As a result, we’re what W.B. Yeats describes as “a soul fastened to a dying animal.” were out intellect unfortunately outscaled our bodies.

- Even though the core goal of a AGI is harmless and aligned to us it is hard to say whether the instrumental sub goals to achieve the end goals are. Again we get the case were an AGI will most likely seek power since its most likely instrumental to the core goal. 

- So Turry didn’t “turn against us” or “switch” from Friendly AI to Unfriendly AI—she just kept doing her thing as she became more and more advanced. Turry's capabilities generalised while the goal did not. 

- Unless one can prove that a AI is generally aligned with human values and the AI is corrigible its near impossible to create an AI that wont at some point behave missaligned since human moral and thus what constitutes an aligned AI always changes with time. 


**Thoughts and summary:**

Tim Urban, founder of the blog [Wait but Why](https://waitbutwhy.com/), wrote this article describing his discovery of the AI safety issue in early 2015.  The piece focuses on philosophical arguments for AI safety and the evolution of human technology rather than  technical details. Urban's casual,  entertaining writing style sets his work apart from similar material I've read. While I'm not deeply familiar with philosophy, I found the article engaging and recommend it to anyone interested in AI safety, human progress, or philosophy – despite some now-outdated elements.  That outdatedness, I'd argue, actually underscores Urban's point.

For instance, Urban highlights the challenges that faced early Artificial Narrow Intelligence (ANI), such as understanding language or classifying images, in contrast to its superhuman abilities in chess and calculation. Remarkably, today's ANI easily accomplishes those image classification tasks, often surpassing humans.  This rapid shift exemplifies the point that technological advancement was hard to grasp even in 2015.


**Key Takeaways:**

* **Law of acceleration:** 
More advanced societies progress faster than less advanced ones due to factors like increased resources, knowledge sharing, and accumulated gains. We tend to underestimate future technological progress because we often anchor our predictions to the current rate of change, rather than accounting for exponential growth.

* **Understanding the scope of AGI/ASI and the scaling of computers:** 
"Nothing will make you appreciate human intelligence like learning about how unbelievably challenging it is to try to create a computer as smart as we are." However, once a computer achieves human-level intelligence (AGI), it gains significant advantages:
  1. Speed - Modern microprocessors are already 10 million times faster then the brain (In neuron activation speed). Allowing the computer to perform the same cognitive task as a human but simply much faster.
  2. Computers surpass humans in scaling potential (RAM, storage, parallel computation, global knowledge access). Physical limitations restrict humans, while computers can endlessly share knowledge, parallelize tasks, and avoid the need for rest. This dramatically accelerates their progress.

* **The *POTENTIAL* incomprehensible intellect of ASI:**
 The Law of Acceleration suggests that ASI may be closer than we think. When coupled with a computer's inherent capacity for scaling intelligence, an AGI's self-learning could quickly lead to ASI.  This could result in an intellect so far beyond our comprehension that it'd be akin to humans observing ants. ASI has the potential to be humanity's final invention, either leading to our extinction or providing solutions like immortality. I strongly recommend reading Tim's thoughts on [the life balance beam](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-2.html#:~:text=First%2C%20looking%20at%20history%2C%20we%20can%20see%20that%20life%20works%20like%20this%3A%20species%20pop%20up%2C%20exist%20for%20a%20while%2C%20and%20after%20some%20time%2C%20inevitably%2C%20they%20fall%20off%20the%20existence%20balance%20beam%20and%20land%20on%20extinction%E2%80%94) for his argument that ASI presents an existential choice between extinction and immortality.

* **The curse of flesh:**
Tim notes that evolution only optimized humans for survival long enough to reproduce and raise offspring – not  for lifespans of 60, 70, or 80+ years. W.B. Yeats' quote, “a soul fastened to a dying animal,” encapsulates this mismatch between our intellect and our physical limitations. The thought of being on the cusp of an intelligence explosion driven by ASI, yet having it arrive potentially a decade too late for my own lifetime, admittedly fills me with a sense of FOMO.  Perhaps the best we can do is to contribute meaningfully to humanity's advancement, even if we don't fully experience the fruits of that labor.


