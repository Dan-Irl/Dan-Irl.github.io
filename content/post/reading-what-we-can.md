---
date: 2024-04-10
draft: false
params:
  author: Dan Johansson
  ShowToc: true
  ShowReadingTime: true
title: Reading What We Can - The Gauntlet
---

While attending an Apart Research: [https://www.apartresearch.com/](https://www.apartresearch.com/) sprint/hackathon, I was introduced by Esben Kran: [https://twitter.com/EsbenKC](https://twitter.com/EsbenKC) to the ["Reading What We Can"](https://readingwhatwecan.com/) challenge. This challenge involves reading a combination of 20 books and articles over 20 days within your chosen learning path. In my case, it's the "ML Engineering & AI Safety" path as defined on the challenge website. The purpose of this challenge is to consume and learn from important material, but also to frame learning as a digestible challenge.

I'll be updating this post with short thoughts and summaries of each reading material as I progress. Hopefully, this inspires someone to take on the challenge or at least explore some of these readings!

## Reading List

* Day 1: [*Preventing an AI-related catastrophe*](https://80000hours.org/problem-profiles/artificial-intelligence/)
* Day 1: [The AI Revolution](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html)















## Day 1: [*Preventing an AI-related catastrophe*](https://80000hours.org/problem-profiles/artificial-intelligence/)

**Thoughts and Summary**

In his article on the 80,000 Hours website, [Benjamin Hilton](https://80000hours.org/author/benjamin-hilton/) introduces fundamental AI safety concepts and explores arguments surrounding the potential for an AI-driven catastrophe. This piece is an invaluable resource for those interested in the field – from technical researchers and policy analysts to the generally curious. Hilton's measured and informative approach makes this a must-read for anyone concerned about the future of humanity, where AI may pose an existential risk.

**Key Takeaways:**

* **Experts Weigh In:** A significant number of experts believe there's a genuine risk of AI causing a catastrophe. Though quantifying this probability is challenging and studies yield varied results, it's a concern meriting serious consideration.
* **The Power Dynamic:** Advanced AI systems could seek increasing power, raising numerous safety issues. Carsmith (2022)'s report on power-seeking AI dives deeper into these dangers ([https://arxiv.org/abs/2206.13353](https://arxiv.org/abs/2206.13353)).
* **Competitive Risks:** Market forces incentivize rapid deployment of powerful AI systems. Prioritizing speed over safety can create hazards.
* **Dual Alignment Challenge:**  Aligning AI systems to our goals is complex. We must also preempt misuse of these systems by individuals or groups with malicious intent.
* **Intelligence ≠ Morality:**  Human-level understanding of ethics does not guarantee an AI agent will act ethically. 

**Hilton's Call to Action:**

1. **Technical AI Safety Research:**  Contribute to ongoing research in ways that fit your skills and resources.
2. **AI Strategy/Policy:**  Help shape responsible AI development through careers that bridge research, policy, and the industry. 







## Day 1: [The AI Revolution](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html)

**Thoughts and summary:**

Tim Urban, founder of the blog [Wait but Why](https://waitbutwhy.com/), wrote this article describing his discovery of the AI safety issue in early 2015.  The piece focuses on philosophical arguments for AI safety and the evolution of human technology rather than  technical details. Urban's casual,  entertaining writing style sets his work apart from similar material I've read. While I'm not deeply familiar with philosophy, I found the article engaging and recommend it to anyone interested in AI safety, human progress, or philosophy – despite some now-outdated elements.  That outdatedness, I'd argue, actually underscores Urban's point.

For instance, Urban highlights the challenges that faced early Artificial Narrow Intelligence (ANI), such as understanding language or classifying images, in contrast to its superhuman abilities in chess and calculation. Remarkably, today's ANI easily accomplishes those image classification tasks, often surpassing humans.  This rapid shift exemplifies the point that technological advancement was hard to grasp even in 2015.


**Key Takeaways:**

* **Law of acceleration:** 
More advanced societies progress faster than less advanced ones due to factors like increased resources, knowledge sharing, and accumulated gains. We tend to underestimate future technological progress because we often anchor our predictions to the current rate of change, rather than accounting for exponential growth.

* **Understanding the scope of AGI/ASI and the scaling of computers:** 
"Nothing will make you appreciate human intelligence like learning about how unbelievably challenging it is to try to create a computer as smart as we are." However, once a computer achieves human-level intelligence (AGI), it gains significant advantages:
  1. Speed - Modern microprocessors are already 10 million times faster then the brain (In neuron activation speed). Allowing the computer to perform the same cognitive task as a human but simply much faster.
  2. Computers surpass humans in scaling potential (RAM, storage, parallel computation, global knowledge access). Physical limitations restrict humans, while computers can endlessly share knowledge, parallelize tasks, and avoid the need for rest. This dramatically accelerates their progress.

* **The *POTENTIAL* incomprehensible intellect of ASI:**
 The Law of Acceleration suggests that ASI may be closer than we think. When coupled with a computer's inherent capacity for scaling intelligence, an AGI's self-learning could quickly lead to ASI.  This could result in an intellect so far beyond our comprehension that it'd be akin to humans observing ants. ASI has the potential to be humanity's final invention, either leading to our extinction or providing solutions like immortality. I strongly recommend reading Tim's thoughts on [the life balance beam](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-2.html#:~:text=First%2C%20looking%20at%20history%2C%20we%20can%20see%20that%20life%20works%20like%20this%3A%20species%20pop%20up%2C%20exist%20for%20a%20while%2C%20and%20after%20some%20time%2C%20inevitably%2C%20they%20fall%20off%20the%20existence%20balance%20beam%20and%20land%20on%20extinction%E2%80%94) for his argument that ASI presents an existential choice between extinction and immortality.

* **The curse of flesh:**
Tim notes that evolution only optimized humans for survival long enough to reproduce and raise offspring – not  for lifespans of 60, 70, or 80+ years. W.B. Yeats' quote, “a soul fastened to a dying animal,” encapsulates this mismatch between our intellect and our physical limitations. The thought of being on the cusp of an intelligence explosion driven by ASI, yet having it arrive potentially a decade too late for my own lifetime, admittedly fills me with a sense of FOMO.  Perhaps the best we can do is to contribute meaningfully to humanity's advancement, even if we don't fully experience the fruits of that labor.


